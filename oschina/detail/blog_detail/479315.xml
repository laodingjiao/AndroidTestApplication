<?xml version="1.0" encoding="UTF-8"?><oschina>
	<blog>
								<id>479315</id>
		<title><![CDATA[mfs分布式文件系统的配置]]></title>
		<url><![CDATA[http://my.oschina.net/u/2368504/blog/479315]]></url>
		<where><![CDATA[工作日志]]></where>
		<commentCount>0</commentCount>
		<body><![CDATA[<style type='text/css'>pre {white-space:pre-wrap;word-wrap:break-word;}</style><p>mfs分布式文件系统，所需主机：<br>管理服务器 managing server (master&nbsp; vm3) <br>元数据日志服务器 Metalogger server(Metalogger)(vm6)<br>数据存储服务器 data servers (chunkservers) （两台vm5 vm6 做负载均衡）<br>客户机挂载使用 client computers<br><br>1.生成 rpm,便于部署:<br>[root@vm3~]# yum install -y fuse-devel zlib-devel gcc rpm-build.x86_64<br>[root@vm3~]# mv mfs-1.6.27-5.tar.gz mfs-1.6.27.tar.gz<br>[root@vm3~]# rpmbuild -tb mfs-1.6.27-5.tar.gz <br>[root@vm3~]# cd rpmbuild/<br>[root@vm3rpmbuild]# ls<br>BUILD&nbsp; BUILDROOT&nbsp; RPMS&nbsp; SOURCES&nbsp; SPECS&nbsp; SRPMS<br>[root@vm3rpmbuild]# cd RPMS/x86_64/<br>[root@vm3x86_64]# ls<br>mfs-cgi-1.6.27-4.x86_64.rpm&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mfs-client-1.6.27-4.x86_64.rpm<br>mfs-cgiserv-1.6.27-4.x86_64.rpm&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mfs-master-1.6.27-4.x86_64.rpm<br>mfs-chunkserver-1.6.27-4.x86_64.rpm&nbsp; mfs-metalogger-1.6.27-4.x86_64.rpm<br><br>2.主控服务器 Master server 安装:<br>[root@vm3x86_64]# rpm -ivh mfs-master-1.6.27-4.x86_64.rpm mfs-cgi*<br>Preparing...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ########################################### [100%]<br>&nbsp;&nbsp; 1:mfs-cgi&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ########################################### [ 33%]<br>&nbsp;&nbsp; 2:mfs-cgiserv&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ########################################### [ 67%]<br>&nbsp;&nbsp; 3:mfs-master&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ########################################### [100%]<br>[root@vm3x86_64]# cd /etc/mfs/<br>[root@vm3mfs]# ls<br>mfsexports.cfg.dist&nbsp; mfsmaster.cfg.dist&nbsp; mfstopology.cfg.dist<br>[root@vm3mfs]# cp mfsexports.cfg.dist mfsexports.cfg<br>[root@vm3mfs]# cp mfsmaster.cfg.dist mfsmaster.cfg<br>[root@vm3mfs]# cp mfstopology.cfg.dist mfstopology.cfg<br>[root@vm3mfs]# vim mfsexports.cfg<br>&nbsp; # Allow "meta".<br>&nbsp; 172.25.254.0/24&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; .&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; rw&nbsp; 允许172.25.254.网段可写<br>[root@vm3mfs]# vim /etc/hosts&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;<br>&nbsp;172.25.254.3 vm3.example.com mfsmaster<br>[root@vm3mfs]# cd /var/lib/mfs/<br>[root@vm3mfs]# cp metadata.mfs.empty metadata.mfs<br>[root@vm3mfs]# chown nobody /var/lib/mfs/ -R<br>[root@vm3mfs]# mfsmaster start<br>[root@vm3mfs]# mfsmaster test<br>mfsmaster pid: 6643<br>[root@vm3mfs]# cd /usr/share/mfscgi/<br>[root@vm3mfscgi]# chmod +x *.cgi<br>[root@vm3mfscgi]# mfscgiserv #启动 CGI 监控服务<br><br>现在再通过浏览器访问 http://172.25.254.3:9425/ 应该可以看见这个 MooseFS 系统的全部信息,包括主控 master 和存储服务 chunkserver 。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;<br>3.配置数据存储服务器 data servers (chunkservers)（vm5 vm6）<br>[root@vm3x86_64]# pwdcd<br>/root/rpmbuild/RPMS/x86_64<br>[root@vm3x86_64]# scp mfs-chunkserver-1.6.27-4.x86_64.rpm 172.25.254.5:<br>[root@vm3x86_64]# scp mfs-chunkserver-1.6.27-4.x86_64.rpm 172.25.254.6:<br><br>切换到vm5<br>[root@vm5~]# rpm -ivh mfs-chunkserver-1.6.27-4.x86_64.rpm <br>Preparing...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ###########################################[100%]<br>&nbsp;1:mfs-chunkserver&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ########################################### [100%]<br>[root@vm5~]# cd /etc/mfs/<br>[root@vm5mfs]# ls<br>mfschunkserver.cfg.dist&nbsp; mfshdd.cfg.dist<br>[root@vm5mfs]# cp mfschunkserver.cfg.dist mfschunkserver.cfg<br>[root@vm5mfs]# cp mfshdd.cfg.dist mfshdd.cfg<br>[root@vm5mfs]# mkdir /var/lib/mfs<br>[root@vm5mfs]# chown nobody /var/lib/mfs/<br>[root@vm5mfs]# vim mfshdd.cfg<br># mount points of HDD drives<br>#<br>#/mnt/hd1<br>#/mnt/hd2<br>#etc.<br>/mnt/chunk1<br>[root@vm5mfs]# mkdir /mnt/chunk1<br>[root@vm5mfs]# chown nobody /mnt/chunk1<br>[root@vm5mfs]# mfschunkserver <br>working directory: /var/lib/mfs<br>lockfile created and locked<br>initializing mfschunkserver modules ...<br>hdd space manager: path to scan: /mnt/chunk1/<br>hdd space manager: start background hdd scanning (searching for available chunks)<br>main server module: listen on *:9422<br>[root@vm5mfs]# vim /etc/hosts<br>加入172.25.254.3 mfsmaster<br><br>在vm6做类似操作：<br>[root@vm6mfs]# vim mfshdd.cfg<br># mount points of HDD drives<br>#<br>#/mnt/hd1<br>#/mnt/hd2<br>#etc.<br>/mnt/chunk2<br><br>4.客户端挂载读取<br>[root@vm3x86_64]# scp mfs-client-1.6.27-4.x86_64.rpm 172.25.254.1:<br>[root@benberba ~]# rpm -ivh mfs-client-1.6.27-4.x86_64.rpm <br>Preparing...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ########################################### [100%]<br>&nbsp;&nbsp; 1:mfs-client&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ########################################### [100%]<br>[root@benberba ~]# cd /etc/mfs/<br>[root@benberba mfs]# ls<br>mfsmount.cfg.dist<br>[root@benberba mfs]# cp mfsmount.cfg.dist mfsmount.cfg<br>[root@benberba mfs]# mkdir /mnt/mfs<br>[root@benberba mfs]# vim mfsmount.cfg<br>mfsmaster=mfsmaster<br>/mnt/mfs<br>[root@benberba mfs]# vim /etc/hosts<br>172.25.254.3 mfsmaster<br>[root@benberba mfs]# mfsmount <br>mfsmaster accepted connection with parameters: read-write,restricted_ip ; root mapped to root:root<br>MFS 测试:<br>在 MFS 挂载点下创建两个目录,并设置其文件存储份数<br>[root@benberba mfs]# cd /mnt/mfs/<br>[root@benberba mfs]# mkdir dir1<br>[root@benberba mfs]# mkdir dir2<br>[root@benberba mfs]# mfssetgoal -r 2 dir2&nbsp; 设置在 dir2 中文件存储份数为两个,默认是一个<br>[root@benberba mfs]# cp /etc/passwd dir1<br>[root@benberba mfs]# cp /etc/passwd dir2<br><br>查看文件信息<br>[root@benberba mfs]# mfsfileinfo dir1/passwd <br>dir1/passwd:<br>&nbsp;&nbsp; &nbsp;chunk 0: 0000000000000001_00000001 / (id:1 ver:1)<br>&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;copy 1: 172.25.254.6:9422<br>[root@benberba mfs]# mfsfileinfo dir2/passwd <br>dir2/passwd:<br>&nbsp;&nbsp; &nbsp;chunk 0: 0000000000000002_00000001 / (id:2 ver:1)<br>&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;copy 1: 172.25.254.5:9422<br>&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;copy 2: 172.25.254.6:9422<br><br>关闭 mfschunkserver2 后再查看文件信息（即[root@vm6mfs]# mfschunkserver stop）<br>[root@benberba mfs]# mfsfileinfo dir1/passwd <br>dir1/passwd:<br>&nbsp;&nbsp; &nbsp;chunk 0: 0000000000000001_00000001 / (id:1 ver:1)<br>&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;no valid copies !!!<br>[root@benberba mfs]# mfsfileinfo dir2/passwd <br>dir2/passwd:<br>&nbsp;&nbsp; &nbsp;chunk 0: 0000000000000002_00000001 / (id:2 ver:1)<br>&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;copy 1: 172.25.254:9422<br>启动 mfschunkserver2 后,文件回复正常（[root@vm6mfs]# mfschunkserver start）。<br>[root@benberba mfs]# mfsfileinfo dir2/passwd <br>dir2/passwd:<br>&nbsp;&nbsp; &nbsp;chunk 0: 0000000000000002_00000001 / (id:2 ver:1)<br>&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;copy 1: 172.25.254.5:9422<br>&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;copy 2: 172.25.254.6:9422<br><br>恢复误删文件<br>[root@benberba mfs]# rm -f dir1/passwd <br>[root@benberba mfs]# mfsgettrashtime dir1/<br>dir1/: 86400<br>文件删除后存放在“ 垃圾箱”中的时间称为隔离时间, 这个时间可以用 mfsgettrashtime 命令来查<br>看,用 mfssettrashtime 命令来设置,单位为秒,默认为 86400 秒。<br>[root@benberba mfs]# mkdir /mnt/mfsmeta<br>[root@benberba mfs]# mfsmount -m /mnt/mfsmeta/ -H mfsmaster<br>mfsmaster accepted connection with parameters: read-write,restricted_ip<br>[root@benberba mfs]# cd /mnt/mfsmeta/trash<br>[root@benberba trash]# ls<br>00000004|dir1|passwd&nbsp; undel<br>[root@benberba trash]# mv 00000004\|dir1\|passwd undel/<br>到 dir1 目录中可以看到 passwd 文件恢复<br>[root@benberba ~]# mfsfileinfo /mnt/mfs/dir1/passwd <br>/mnt/mfs/dir1/passwd:<br>&nbsp;&nbsp; &nbsp;chunk 0: 0000000000000001_00000001 / (id:1 ver:1)<br>&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;copy 1: 172.25.254.6:9422<br>在 MFSMETA 的目录里,除了 trash 和 trash/undel 两个目录,还有第三个目录 reserved,该目<br>录内有已经删除的文件,但却被其他用户一直打开着。在用户关闭了这些被打开的文件后,<br>reserved 目录中的文件将被删除,文件的数据也将被立即删除。此目录不能进行操作</p> 
<p>为了安全停止 MooseFS 集群,建议执行如下的步骤:<br># umount -l /mnt/mfs&nbsp;&nbsp;&nbsp;&nbsp; #客户端卸载 MooseFS 文件系统<br># mfschunkserver stop&nbsp; #停止 chunk server 进程<br># mfsmetalogger stop&nbsp;&nbsp; #停止 metalogger 进程<br># mfsmaster stop&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #停止主控 master server 进程<br>安全的启动 MooseFS 集群:<br># mfsmaster start&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #启动 master 进程<br># mfschunkserver start&nbsp; #启动 chunkserver 进程<br># mfsmetalogger start&nbsp;&nbsp;&nbsp; #启动 metalogger 进程<br># mfsmount&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #客户端挂载 MooseFS 文件系统<br>实际上无论如何顺序启动或关闭,未见任何异常,master 启动后,metalogger、chunker、client<br>三个元素都能自动与 master 建立连接。<br><br>故障测试:<br>Client 客户端断电、断网对 MFS 的体系不产生影响.<br>如果客户端误杀 killall -9 mfsmount 进程,需要先 umount /mnt/mfs,然后再 mfsmount。否则会<br>提示:/mnt/mfs: Transport endpoint is not connected<br></p> 
<p><br></p> 
<p>mfschunkserver:<br></p> 
<p>断网、杀掉 mfschunkserver 程序对 MFS 系统无影响。<br>断电:<br>#无文件传输时,对两个 chunker 都无影响;<br>#当有文件传输时,但是文件设置存储一份时,对文件的存储无影响。<br>#文件设置存储两份,数据传输过程中,关掉 chunker1,等待数据传输完毕后,启动<br>chunker1.chunker1 启动后,会自动从 chunker2 复制数据块。整个过程中文件访问不受影响。<br>#文件设置存储两份,数据传输过程中,关掉 chunker1,不等待数据传输完毕,开机启动<br>chunker1.chunker1 启动后,client 端会向 chunker1 传输数据,同时 chunker1 也从 chunker2 复<br>制缺失的块。<br>只要不是两个 chunker 服务器同时挂掉的话,就不会影响文件的传输,也不会影响服务的使用。<br><br>master 端:<br>断网、杀掉 MFS 的 master 服务对 MFS 系统无影响。<br>断电可能会出现以下的情况:<br>#当没有文件传输时,可在服务器重启之后,运行 mfsmetarestore –a 进行修复,之后执行<br>mfsmaster start 恢复 master 服务。<br># mfsmetarestore -a<br>loading objects (files,directories,etc.) ... ok<br>loading names ... ok<br>loading deletion timestamps ... ok<br>loading chunks data ... ok<br>checking filesystem consistency ... ok<br>connecting files and chunks ... ok<br>store metadata into file: /var/lib/mfs/metadata.mfs<br># mfsmaster start<br>working directory: /var/lib/mfs<br>lockfile created and locked<br>initializing mfsmaster modules ...<br>loading sessions ... ok<br>sessions file has been loaded<br>exports file has been loaded<br>mfstopology configuration file (/etc/mfstopology.cfg) not found - using defaults<br>loading metadata ...<br>loading objects (files,directories,etc.) ... ok<br>loading names ... ok<br>loading deletion timestamps ... ok<br>loading chunks data ... ok<br>checking filesystem consistency ... ok<br>connecting files and chunks ... ok<br>all inodes: 5<br>directory inodes: 3<br>file inodes: 2<br>chunks: 2<br>metadata file has been loaded<br>stats file has been loaded<br>master &lt;-&gt; metaloggers module: listen on *:9419<br>master &lt;-&gt; chunkservers module: listen on *:9420<br>main master server module: listen on *:9421<br>mfsmaster daemon initialized properly<br>#当有文件传输时,可能会在/usr/local/mfs/sbin/mfsmetarestore –a 进行修复时可能会出现:<br># mfsmetarestore -a<br>loading objects (files,directories,etc.) ... ok<br>loading names ... ok<br>loading deletion timestamps ... ok<br>loading chunks data ... ok<br>checking filesystem consistency ... ok<br>connecting files and chunks ... ok<br>�S:115: error: 32 (Data mismatch)<br>此时无法修复也无法启动 master 服务,有个应急的办法是将 metadata.mfs.back 复制成<br>metadata.mfs,然后再启动 master。这样将会丢失那些正在传输的数据。<br><br></p> 
<p><br></p> 
<p><br></p> 
<p><br></p> 
<p><br></p>]]></body>
		<author><![CDATA[benberba]]></author>
		<authorid>2368504</authorid>
		<documentType>1</documentType>
        <pubDate>2015-07-16 14:46:19</pubDate>
		<favorite>0</favorite>
			</blog>
</oschina>